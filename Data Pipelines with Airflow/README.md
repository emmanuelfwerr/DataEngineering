# Data Pipelines - using Airflow

## Project Overview
A music streaming company, Sparkify, has decided that it is time to introduce more automation and monitoring to their data warehouse ETL pipelines and come to the conclusion that the best tool to achieve this is Apache Airflow. They have decided to bring the data engineering team into the project and expect us to create high grade data pipelines that are dynamic and built from reusable tasks, can be monitored, and allow easy backfills. They have also noted that the data quality plays a big part when analyses are executed on top the data warehouse and want to run tests against their datasets after the ETL steps have been executed to catch any discrepancies in the datasets.

In this project, we are tasked with creating high grade data pipelines that are dynamic and built from reusable tasks, can be monitored, and allow easy backfills. Tests need to run against their datasets after the ETL steps have been executed to catch any discrepancies in the datasets.

### Environment

This project requires **Python 3.8** and the following Python libraries installed:

- [airflow](https://airflow.apache.org/docs/apache-airflow/stable/python-api-ref.html#)
- [datetime](https://docs.python.org/3/library/datetime.html)
- [os](https://docs.python.org/3/library/os.html)
- [sql](https://docs.python.org/3/library/sqlite3.html)

### Index

- `dags`
  - `pipeline.py`: Defines main DAG and performs all tasks in specified order
  - `sql.py`: Contains SQL create table and insert statements
  - `create_tables.sql`: contains SQL create table statements 
- `plugins/helpers`
  - `sql_queries.py`: Contains SQL queries for the pipeline
- `plugins/operators`
  - `data_quality.py`: Defines `DataQualityOperator` to run data quality checks on all tables passed as parameter
  - `load_dimensions.py`: Defines `LoadDimensionOperator` to load a dimension table from staging tables
  - `load_fact.py`: Defines `LoadFactOperator` to load fact table from staging tables
  - `stage_redshift.py`: Defines `StageToRedshiftOperator` to copy JSON data from S3 to staging tables in Redshift

### Data

The source data is in log files given the Amazon S3 bucket.

* Song data: `s3://udacity-dend/song_data`
* Log data: `s3://udacity-dend/log_data`
* Log data json path: `s3://udacity-dend/log_json_path.json`
    
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON 
format and contains metadata about a song and the artist of that song. The files are partitioned 
by the first three letters of each song's track ID. 

Log files contains songplay events of the users in json format 
while song_data contains list of songs details.

The second dataset consists of log files in JSON format generated by this event simulator based 
on the songs in the dataset above. These simulate app activity logs from an imaginary music 
streaming app based on configuration settings. The log files in the dataset are partitioned by 
year and month.

### Instructions to Run

* Create AWS Redshift db
* Run `statments in sql.py`
    * This will create the database and all the required tables
* Run `pipeline.py`
    * This will start pipeline and process the entire datasets

